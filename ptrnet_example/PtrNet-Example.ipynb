{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "max_length = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from pprint import pprint\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# See https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264\n",
    "\n",
    "# Uncomment this to stop corner printing and see full/verbatim\n",
    "#np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "\n",
    "def generate_nested_sequence(length, min_seglen=5, max_seglen=10):\n",
    "    \"\"\"Generate low-high-low sequence, with indexes of the first/last high/middle elements\"\"\"\n",
    "\n",
    "    # Low (1-5) vs. High (6-10)\n",
    "    seq_before = [(random.randint(1,5)) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq_during = [(random.randint(6,10)) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq_after = [random.randint(1,5) for x in range(random.randint(min_seglen, max_seglen))]\n",
    "    seq = seq_before + seq_during + seq_after\n",
    "\n",
    "    # Pad it up to max len with 0's\n",
    "    seq = seq + ([0] * (length - len(seq)))\n",
    "    return [seq, len(seq_before), len(seq_before) + len(seq_during)-1]\n",
    "\n",
    "\n",
    "def create_one_hot(length, index):\n",
    "    \"\"\"Returns 1 at the index positions; can be scaled by client\"\"\"\n",
    "    a = np.zeros([length])\n",
    "    a[index] = 1.0\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_lstm_state(cell):\n",
    "    \"\"\"Centralize definition of 'state', to swap .c and .h if desired\"\"\"\n",
    "    return cell.c\n",
    "\n",
    "\n",
    "def print_pointer(arr, first, second):\n",
    "    \"\"\"Pretty print the array, along with pointers to the first/second indices\"\"\"\n",
    "    first_string = \" \".join([(\" \" * (2 - len(str(x))) + str(x)) for x in arr])\n",
    "    print(first_string)\n",
    "    second_array = [\"  \"] * len(arr)\n",
    "    second_array[first] = \"^1\"\n",
    "    second_array[second] = \"^2\"\n",
    "    if (first == second):\n",
    "        second_array[first] = \"^B\"\n",
    "    second_string = \" \" + \" \".join([x for x in second_array])\n",
    "    print(second_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for i in generate_nested_sequence(40)[0]:\n",
    "    x.append(create_one_hot(40, i))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_of_indices, blend_dim, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_of_indices = num_of_indices\n",
    "        self.blend_dim = blend_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encode = nn.LSTMCell(input_dim, hidden_size)\n",
    "        self.decode = nn.LSTMCell(input_dim, hidden_size)\n",
    "        self.blend_decoder = nn.Linear(hidden_size, blend_dim)\n",
    "        self.blend_encoder = nn.Linear(hidden_size, blend_dim)\n",
    "        self.scale_blend = nn.Linear(blend_dim, input_dim)\n",
    "        \n",
    "    def zero_hidden_state(self):\n",
    "        return Variable(torch.zeros([self.batch_size, self.hidden_size]).cuda())\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #TODO - zero \n",
    "        hidden = self.zero_hidden_state()\n",
    "        cell_state = self.zero_hidden_state()\n",
    "        encoder_states = []\n",
    "        for j in range(len(inp[0])):\n",
    "            encoder_input = inp[:, j:j+1]\n",
    "            hidden, cell_state = self.encode(encoder_input, (hidden, cell_state)) \n",
    "            encoder_states.append(cell_state)\n",
    "            \n",
    "        decoder_state = encoder_states[-1]\n",
    "        pointers = []\n",
    "        pointer_distributions = []\n",
    "        \n",
    "        start_token = 20\n",
    "        decoder_input = Variable(torch.Tensor([start_token] * self.batch_size)\n",
    "                                 .view(self.batch_size, self.input_dim).cuda())\n",
    "\n",
    "        for i in range(self.num_of_indices):\n",
    "            hidden = self.zero_hidden_state()\n",
    "            cell_state = self.zero_hidden_state()\n",
    "            hidden, cell_state = self.decode(decoder_input, (hidden, cell_state))\n",
    "            \n",
    "            decoder_blend = self.blend_decoder(cell_state)\n",
    "            encoder_blends = []\n",
    "            index_predists = []\n",
    "            for i in range(len(inp[0])):\n",
    "                encoder_blend = self.blend_encoder(encoder_states[i])\n",
    "                raw_blend = encoder_blend + decoder_blend\n",
    "                scaled_blend = self.scale_blend(raw_blend).squeeze(1)\n",
    "                \n",
    "                index_predist = scaled_blend\n",
    "                \n",
    "                encoder_blends.append(encoder_blend)\n",
    "                index_predists.append(index_predist)\n",
    "                \n",
    "            index_predistribution = torch.stack(index_predists).t()\n",
    "            index_distribution = F.softmax(index_predistribution)\n",
    "            pointer_distributions.append(index_distribution)\n",
    "            index = index_distribution.data.max(1)[1].squeeze(1)\n",
    "\n",
    "            emb = embedding_lookup(inp.t(), Variable(index))\n",
    "            pointer_raw = torch.diag(emb)\n",
    "            pointer = pointer_raw\n",
    "\n",
    "            pointers.append(pointer)\n",
    "            decoder_input = pointer.unsqueeze(1)\n",
    "            \n",
    "            #print('pointer: {}'.format(pointers))\n",
    "        index_distributions = torch.stack(pointer_distributions)\n",
    "        return index_distributions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "def train(epochs, model,  train_batches, print_every = 100):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.1)\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (data, target) in enumerate(train_batches):\n",
    "                data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "                index_distributions = model(data)\n",
    "                loss = torch.sqrt(\n",
    "                            torch.mean(\n",
    "                                torch.pow(index_distributions - target, 2)\n",
    "                            )\n",
    "                )\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if epoch % print_every == 0:\n",
    "                print('epoch: {} -- loss: {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_lookup(embeddings, indices):\n",
    "    result =  embeddings.index_select(0, indices.view(-1))\n",
    "    return result.view(*(indices.size() + embeddings.size()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segment_length_min = 11\n",
    "train_segment_length_max = 20\n",
    "\n",
    "seqs = []\n",
    "start_indices = []\n",
    "end_indices = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    seq, start, end = generate_nested_sequence(max_length, \n",
    "                                                train_segment_length_min, \n",
    "                                                train_segment_length_max)\n",
    "    \n",
    "    start_, end_ = create_one_hot(max_length, start),  create_one_hot(max_length, end)\n",
    "    seqs.append(seq), start_indices.append(start_), end_indices.append(end_)\n",
    "\n",
    "#print(len(seqs))\n",
    "#pprint([len(seq) for seq in seqs])\n",
    "    \n",
    "seqs          = torch.Tensor(seqs)\n",
    "start_indices = torch.Tensor(start_indices)\n",
    "end_indices   = torch.Tensor(end_indices)\n",
    "indices = torch.stack([start_indices, end_indices])\n",
    "train_batches = [(seqs, indices),]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_input, sample_output = train_batches[0]\n",
    "model = Model(1, 6, 2, 6, batch_size)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(40000, model, train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segment_length_min = 5\n",
    "test_segment_length_max = 10\n",
    "\n",
    "seqs = []\n",
    "start_indices = []\n",
    "end_indices = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    seq, start, end = generate_nested_sequence(max_length, \n",
    "                                                test_segment_length_min, \n",
    "                                                test_segment_length_max)\n",
    "    \n",
    "    start_, end_ = create_one_hot(max_length, start),  create_one_hot(max_length, end)\n",
    "    seqs.append(seq), start_indices.append(start_), end_indices.append(end_)\n",
    "\n",
    "#print(len(seqs))\n",
    "#pprint([len(seq) for seq in seqs])\n",
    "    \n",
    "seqs          = torch.Tensor(seqs)\n",
    "start_indices = torch.Tensor(start_indices)\n",
    "end_indices   = torch.Tensor(end_indices)\n",
    "indices = torch.stack([start_indices, end_indices])\n",
    "test_batches = [(seqs, indices),]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in test_batches:\n",
    "    data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "    index_distributions = model(data)\n",
    "    loss = torch.sqrt(\n",
    "                torch.mean(\n",
    "                    torch.pow(index_distributions - target, 2)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    print('loss: {}'.format(loss.data[0]))\n",
    "\n",
    "print(data[:10])\n",
    "print(target[0].data.max(1)[1][:10])\n",
    "print(target[1].data.max(1)[1][:10])\n",
    "print(index_distributions[0].data.max(1)[1][:10])\n",
    "print(index_distributions[1].data.max(1)[1][:10])\n",
    "incorrect_pointers = 0\n",
    "\n",
    "results = index_distributions.data.numpy()\n",
    "print(index_distributions)\n",
    "for batch_index in range(batch_size):\n",
    "    if batch_index >= 59:\n",
    "        break\n",
    "    print(results[0][batch_index][1])\n",
    "    first_diff = start_[batch_index] - results[1][batch_index][0]\n",
    "    first_diff_max = np.max(np.abs(first_diff))\n",
    "    print(first_diff, first_diff_max)\n",
    "    first_ptr = np.argmax(results[1][batch_index][0])\n",
    "    if first_diff_max >= .5:  # bit stricter than argmax but let's hold ourselves to high standards, people\n",
    "        incorrect_pointers += 1\n",
    "    second_diff = end_[batch_index] - results[1][batch_index][1]\n",
    "    second_diff_max = np.max(np.abs(second_diff))\n",
    "    second_ptr = np.argmax(results[1][batch_index][1])\n",
    "    if second_diff_max >= .5:\n",
    "        incorrect_pointers += 1\n",
    "\n",
    "    print_pointer(seqs[batch_index], first_ptr, second_ptr)\n",
    "    #print(\"\")\n",
    "\n",
    "test_pct = np.round(100.0 * ((2 * batch_size) - incorrect_pointers) / (2 * batch_size), 5)\n",
    "print(\"\")\n",
    "print(\" %s / %s (correct/total); test pct %s\" % ((2*batch_size) - incorrect_pointers,\n",
    "                                                 2 * batch_size,\n",
    "test_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
